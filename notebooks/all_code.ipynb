{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rz7xHIEy4JEv"
      },
      "outputs": [],
      "source": [
        "# data collection with snscrape\n",
        "!pip install snscrape\n",
        "!pip install --upgrade snscrape\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "import pandas as pd\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = [\"윤석열 언론\", \"언론 자유 한국\", \"언론자유\", \"출판의 자유\",\n",
        "            \"방송 자유 한국\", \"대통령 언론 한국\", \"기자 자유 한국\",\n",
        "            \"대통령 방송 한국\", \"윤석열 방송 자유\", \"윤석열 기자 자유\",\n",
        "            \"정부 언론 한국\", \"대통령실 언론 한국\", \"정부 기자 한국\",\n",
        "            \"정부 신문 자유 한국\", \"대통령 신문 자유\", \"신문 자유 한국\",\n",
        "            \"윤석열 신문 자유\"]\n",
        "\n",
        "tweets_list = []\n",
        "\n",
        "for keyword in keywords:\n",
        "    for i, tweet in enumerate(sntwitter.TwitterSearchScraper(f'\"{keyword}\" since:2022-05-10 until:2023-04-10').get_items()):\n",
        "        if i > 1000000:\n",
        "            break\n",
        "        tweets_list.append([tweet.date, tweet.id, tweet.rawContent, tweet.user.username])"
      ],
      "metadata": {
        "id": "vDNJc0qg35e-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Id', 'Text', 'Username'])"
      ],
      "metadata": {
        "id": "D1FGrpqF4qA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing tweets\n",
        "tweets_df = tweets_df[~tweets_df['Text'].str.contains('마스토돈|힐러리|신냉전|펜스|도널드|플로리다|POTUS|공화당|달러|민생|아제르바이잔|절대왕정|하토야마|태슬라|맹진|정부용어사전|안철수|적자|총파업|화물연대|대우조선|인플레이션|조문|스코틀랜드|강제동원|대선후보자격검증|부산수요시위|은행권|부동산|기념|칸|국제영화제|소득|예산|미사일|무인기|맨해튼|우크라이나|EricTrump|스티브|머스크|FBI|비핵화|독도|이세창|간첩|엘리자베스|대만|집값|방역|러시아|마약과의|백신|멍게|아이히만|119동물구조대상|동물자유연대|동물학대|풍산개|반혁명분자들과|부정선거|지방선거|정론직필|격노')]"
      ],
      "metadata": {
        "id": "9CaAlNwp4Qdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_count = tweets_df.Id.nunique()\n",
        "print(f'{unique_count} tweets are unique out of our {tweets_df.shape[0]} total tweets')\n",
        "nodupl_tweets_df = tweets_df.drop_duplicates(keep='first')\n",
        "print(len(nodupl_tweets_df))"
      ],
      "metadata": {
        "id": "CaLIy0nD4Ran"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy\n",
        "!sudo apt-get install g++ openjdk-8-jdk python3-dev python3-pip curl\n",
        "!python3 -m pip install --upgrade pip\n",
        "!python3 -m pip install konlpy\n",
        "!sudo apt-get install curl git\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "tweets_nouns = [okt.nouns(line) for line in nodupl_tweets_df['Text']]"
      ],
      "metadata": {
        "id": "RtRIydgP4SyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = ['무릅쓰', '씨', '류', '기자수집가', '발톱', '딴지', '중', '한마디', '직','원전', '깨', '링크', '주민', '좀', '파도', '라며', '저', '출처', '일', '절', '하루', '전', '주진우', '차', '핵', '취소', '타운', '신고', '사이비', '또', '아', '곳', '공장','출근길','하이킥', '개','생각', '사람', '똑바로', '짓', '사업', '주민', '시작','제목', '얼마나', '곳', '노골', '모두','말', '문건', '이제', '누구', '소리', '여성', '그거', '은', '무슨', '내', '채용', '정도', '고','조선일보', '정말', '알', '때문', '뻔', '미디어랩', '끝', '자기', '이건', '데', '거', '건', '거기', '그냥','네이버', '하나', '위', '오후', '림', '다음', '유키오', '점', '선', '타임즈', '앞', '선', '비', '분', '수소', '뉴시스','하자', '거지', '진짜', '무엇', '며', '못', '오늘', '이번', '동아일보', '해', '줄', '질', '그날', '보', '뿐', '만', '타고', '간질', '종합', '이', '수', '왜', '우리', '아마', '니', '종말', '저리', '누가', '입', '유족', '뷰', '다른', '굥','힘','더', '명', '말','것', '이', '그', '나' ,'제', '게', '재','의','로', '뭐','등','관','를','스픽스', '아치', '롱', '마크', '회', '너', '걸', '너', '안', '때', '민', '지금', '외전']\n",
        "cl_tweets_nouns = [[y for y in x if y not in stop_words] for x in tweets_nouns]"
      ],
      "metadata": {
        "id": "EdNMho4v4VzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracting keywords with tf-idf\n",
        "# TF\n",
        "from collections import Counter\n",
        "def get_rtfs(article):\n",
        "    terms = [w for doc in cl_tweets_nouns for w in doc]\n",
        "    tfs = Counter(terms)\n",
        "    return tfs\n",
        "rtfs = get_rtfs(cl_tweets_nouns)\n",
        "rtfs\n",
        "\n",
        "from math import log10\n",
        "def get_tfs(docs):\n",
        "    terms = [w for doc in cl_tweets_nouns for w in doc]\n",
        "    tfs = Counter(terms)\n",
        "    tfs = {t:log10(f + 1) for t, f in rtfs.items()}\n",
        "    return tfs\n",
        "tfs = get_tfs(cl_tweets_nouns)\n",
        "tfs\n",
        "\n",
        "# DF\n",
        "def get_dfs(docs, terms):\n",
        "    dfs = {term: 0 for term in terms}\n",
        "\n",
        "    for term in terms:\n",
        "        for doc in docs:\n",
        "            if term in doc:\n",
        "                dfs[term] = dfs.get(term, 0) + 1\n",
        "    return dfs\n",
        "dfs = get_dfs(cl_tweets_nouns, tfs.keys())\n",
        "dfs\n",
        "\n",
        "# IDF\n",
        "def get_idfs(N, dfs, terms):\n",
        "    #every term\n",
        "    idfs = {term: log10(N/dfs[term]) for term in terms}\n",
        "    return idfs\n",
        "N = len(cl_tweets_nouns)\n",
        "idfs = get_idfs(N, dfs, tfs.keys())\n",
        "idfs\n",
        "ordered_idfs = sorted(idfs.items(), key=lambda each:each[1], reverse=True)\n",
        "\n",
        "# TF-IDF\n",
        "def get_tf_idfs(idfs, tfs):\n",
        "    tf_idf = {}\n",
        "    for term in tfs.keys():\n",
        "        tf = tfs.get(term, 0)\n",
        "        idf = idfs.get(term, 0)\n",
        "        tf_idf[term] = tf*idf\n",
        "    return tf_idf\n",
        "get_tf_idfs(idfs, tfs)\n",
        "\n",
        "ordered_tf_idfs = sorted(get_tf_idfs(idfs, tfs).items(), key=lambda each:each[1], reverse=True)\n",
        "ordered_tf_idfs[:30]"
      ],
      "metadata": {
        "id": "cUdBTLKa4WxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# topic modeling with lda\n",
        "!pip install lda\n",
        "import lda\n",
        "import re\n",
        "import numpy as np\n",
        "from pprint import pprint\n",
        "import gensim, spacy, logging, warnings\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "id2word = corpora.Dictionary(cl_tweets_nouns)\n",
        "corpus = [id2word.doc2bow(text) for text in cl_tweets_nouns]\n",
        "n_topics =10\n",
        "\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=n_topics,\n",
        "                                           random_state=100,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=10,\n",
        "                                           passes=10,\n",
        "                                           alpha='symmetric',\n",
        "                                           iterations=100,\n",
        "                                           per_word_topics=True)\n",
        "\n",
        "n_words = 15\n",
        "pprint(lda_model.print_topics(num_topics=n_topics, num_words=n_words))"
      ],
      "metadata": {
        "id": "K1WSpy4c4Z-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pyLDAvis visualization\n",
        "!pip install pyLDAvis\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models\n",
        "import gensim\n",
        "\n",
        "from gensim.utils import SaveLoad\n",
        "gensim.corpora.MmCorpus.serialize('corpus.mm', corpus)\n",
        "id2word.save('corpus.dict')\n",
        "lda_model.save('lda_model.gensim')\n",
        "\n",
        "corpus = gensim.corpora.MmCorpus('corpus.mm')\n",
        "id2word = gensim.corpora.Dictionary.load('corpus.dict')\n",
        "lda_model = gensim.models.ldamodel.LdaModel.load('lda_model.gensim')\n",
        "\n",
        "vis_data = pyLDAvis.gensim_models.prepare(lda_model, corpus, id2word)\n",
        "pyLDAvis.display(vis_data)"
      ],
      "metadata": {
        "id": "ckowUu0W4c48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### word2vec and K-means\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "docs = tweets_df['Text'].values\n",
        "model = Word2Vec(sentences=cl_tweets_nouns, vector_size=100, workers=1, seed=SEED)\n",
        "def vectorize(list_of_docs, model):\n",
        "    features = []\n",
        "\n",
        "    for tokens in list_of_docs:\n",
        "        zero_vector = np.zeros(model.vector_size)\n",
        "        vectors = []\n",
        "        for token in tokens:\n",
        "            if token in model.wv:\n",
        "                try:\n",
        "                    vectors.append(model.wv[token])\n",
        "                except KeyError:\n",
        "                    continue\n",
        "        if vectors:\n",
        "            vectors = np.asarray(vectors)\n",
        "            avg_vec = vectors.mean(axis=0)\n",
        "            features.append(avg_vec)\n",
        "        else:\n",
        "            features.append(zero_vector)\n",
        "    return features\n",
        "\n",
        "vectorized_docs = vectorize(cl_tweets_nouns, model=model)\n",
        "len(vectorized_docs), len(vectorized_docs[0])\n",
        "\n",
        "def mbkmeans_clusters(\n",
        "\tX,\n",
        "    k,\n",
        "    mb,\n",
        "    print_silhouette_values,\n",
        "):\n",
        "    km = MiniBatchKMeans(n_clusters=k, batch_size=mb).fit(X)\n",
        "    print(f\"For n_clusters = {k}\")\n",
        "    print(f\"Silhouette coefficient: {silhouette_score(X, km.labels_):0.2f}\")\n",
        "    print(f\"Inertia:{km.inertia_}\")\n",
        "\n",
        "    if print_silhouette_values:\n",
        "        sample_silhouette_values = silhouette_samples(X, km.labels_)\n",
        "        print(f\"Silhouette values:\")\n",
        "        silhouette_values = []\n",
        "        for i in range(k):\n",
        "            cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]\n",
        "            silhouette_values.append(\n",
        "                (\n",
        "                    i,\n",
        "                    cluster_silhouette_values.shape[0],\n",
        "                    cluster_silhouette_values.mean(),\n",
        "                    cluster_silhouette_values.min(),\n",
        "                    cluster_silhouette_values.max(),\n",
        "                )\n",
        "            )\n",
        "        silhouette_values = sorted(\n",
        "            silhouette_values, key=lambda tup: tup[2], reverse=True\n",
        "        )\n",
        "        for s in silhouette_values:\n",
        "            print(\n",
        "                f\"    Cluster {s[0]}: Size:{s[1]} | Avg:{s[2]:.2f} | Min:{s[3]:.2f} | Max: {s[4]:.2f}\"\n",
        "            )\n",
        "    return km, km.labels_\n",
        "clustering, cluster_labels = mbkmeans_clusters(\n",
        "\tX=vectorized_docs,\n",
        "    k=15,\n",
        "    mb=80,\n",
        "    print_silhouette_values=True,\n",
        ")\n",
        "df_clusters = pd.DataFrame({\n",
        "    \"text\": docs,\n",
        "    \"tokens\": [\" \".join(text) for text in cl_tweets_nouns],\n",
        "    \"cluster\": cluster_labels\n",
        "})\n",
        "\n",
        "print(\"Most representative terms per cluster (based on centroids):\")\n",
        "for i in range(15):\n",
        "    tokens_per_cluster = \"\"\n",
        "    most_representative = model.wv.most_similar(positive=[clustering.cluster_centers_[i]], topn=5)\n",
        "    for t in most_representative:\n",
        "        tokens_per_cluster += f\"{t[0]} \"\n",
        "    print(f\"Cluster {i}: {tokens_per_cluster}\")"
      ],
      "metadata": {
        "id": "ZFESiYXc4hTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# most representative tweets of each cluster\n",
        "test_cluster = 12\n",
        "most_representative_docs = np.argsort(\n",
        "    np.linalg.norm(vectorized_docs - clustering.cluster_centers_[test_cluster], axis=1)\n",
        ")\n",
        "for d in most_representative_docs[:3]:\n",
        "    print(docs[d])\n",
        "    print(\"-------------\")"
      ],
      "metadata": {
        "id": "C_nsbtCF41yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# similar words with word2vec\n",
        "model.wv.most_similar(\"탄압\")"
      ],
      "metadata": {
        "id": "uw3HBwF14eao"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
